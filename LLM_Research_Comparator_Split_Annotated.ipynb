{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28d5e41",
   "metadata": {},
   "source": [
    "# üß† LLM Deep Research Comparator (ChatGPT + Gemini)\n",
    "A notebook to compare answers from GPT-4 and Gemini models for deep research tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba7c49",
   "metadata": {},
   "source": [
    "## üì¶ Imports and Setup\n",
    "This cell imports necessary libraries and loads your API keys from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d97ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import tiktoken\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e87fe",
   "metadata": {},
   "source": [
    "## üîß Model Configuration\n",
    "Specify which OpenAI and Gemini models you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODELS = [\"gpt-4o\", \"gpt-4-mini-high\"]\n",
    "GEMINI_MODEL = \"gemini-pro\"  # You can also try: \"gemini-1.5-pro-latest\" if supported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9c795",
   "metadata": {},
   "source": [
    "## üìù Initial Research Prompt\n",
    "Enter your initial research question or task here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb187729",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "i am a senior economist at the GLA, being sent on this course (https://bse.eu/summer-school/crei-macroeconomics/quantitative-methods-spatial-economics) to improve my quantitative spatial modelling skills... [truncated for brevity]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565a45a",
   "metadata": {},
   "source": [
    "## üî¢ Token Counter\n",
    "Helper function to estimate token count (OpenAI only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156598e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc6825",
   "metadata": {},
   "source": [
    "## üîå OpenAI Query Function\n",
    "Sends prompt to OpenAI (GPT-4, GPT-4-mini) using the new v1+ client interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(prompt, model, key):\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        usage = response.usage.total_tokens if hasattr(response, 'usage') else count_tokens(prompt, model)\n",
    "        cost = usage * (0.00001 if model == \"gpt-4-mini-high\" else 0.00005)\n",
    "        return content, usage, cost\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI Error: {str(e)}\", 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb991a",
   "metadata": {},
   "source": [
    "## üîå Gemini Query Function\n",
    "Sends prompt to Gemini using the public API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gemini(prompt, key, model=\"gemini-pro\"):\n",
    "    try:\n",
    "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        data = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        content = response.json()['candidates'][0]['content']['parts'][0]['text']\n",
    "        tokens = count_tokens(prompt, \"gpt-4o\")  # Approximate\n",
    "        return content, tokens, 0\n",
    "    except Exception as e:\n",
    "        return f\"Gemini Error: {str(e)}\", 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784af30",
   "metadata": {},
   "source": [
    "## üöÄ Run Prompt Across Models\n",
    "This function submits the prompt to all configured models and collects results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ecf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_round(prompt_text, label=\"Initial\"):\n",
    "    results = []\n",
    "\n",
    "    for model in OPENAI_MODELS:\n",
    "        answer, tokens, cost = query_openai(prompt_text, model, OPENAI_KEY)\n",
    "        results.append({\"model\": model, \"provider\": \"OpenAI\", \"response\": answer, \"tokens\": tokens, \"cost\": cost, \"label\": label})\n",
    "\n",
    "    gemini_ans, tokens, cost = query_gemini(prompt_text, GEMINI_KEY, model=GEMINI_MODEL)\n",
    "    results.append({\"model\": GEMINI_MODEL, \"provider\": \"Google\", \"response\": gemini_ans, \"tokens\": tokens, \"cost\": cost, \"label\": label})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e93cd5",
   "metadata": {},
   "source": [
    "## üì• Run Initial Prompt\n",
    "Runs your prompt and shows all model responses side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = run_round(initial_prompt, label=\"Initial\")\n",
    "\n",
    "for _, row in all_responses.iterrows():\n",
    "    display(Markdown(f\"## ü§ñ {row['provider']} ‚Äì {row['model']} ({row['label']})\"))\n",
    "    display(Markdown(f\"**Tokens:** {row['tokens']} | **Est. Cost ($):** {row['cost']:.5f}\"))\n",
    "    display(Markdown(row['response'][:10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752825a3",
   "metadata": {},
   "source": [
    "## üîÑ Optional Follow-Up Prompt\n",
    "Allows you to enter a follow-up and rerun across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up = input(\"\\nType a follow-up prompt or press Enter to skip: \")\n",
    "if follow_up.strip():\n",
    "    follow_responses = run_round(follow_up, label=\"Follow-up\")\n",
    "    all_responses = pd.concat([all_responses, follow_responses], ignore_index=True)\n",
    "    for _, row in follow_responses.iterrows():\n",
    "        display(Markdown(f\"## üîÅ {row['provider']} ‚Äì {row['model']} ({row['label']})\"))\n",
    "        display(Markdown(f\"**Tokens:** {row['tokens']} | **Est. Cost ($):** {row['cost']:.5f}\"))\n",
    "        display(Markdown(row['response'][:10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985998b",
   "metadata": {},
   "source": [
    "## üìù Export Results to Markdown\n",
    "Saves everything to a Markdown file you can import into Notion or Obsidian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = \"\"\n",
    "for _, row in all_responses.iterrows():\n",
    "    md += f\"\\n\\n## ü§ñ {row['provider']} ‚Äì {row['model']} ({row['label']})\\n\"\n",
    "    md += f\"**Tokens:** {row['tokens']} | **Est. Cost ($):** {row['cost']:.5f}\\n\\n\"\n",
    "    md += row['response']\n",
    "\n",
    "with open(\"llm_comparison.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "print(\"\\n‚úÖ All responses saved to llm_comparison.md with token and cost tracking.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
